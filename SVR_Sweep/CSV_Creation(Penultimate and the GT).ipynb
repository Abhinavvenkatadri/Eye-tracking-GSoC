{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36603ce-062e-4a9e-a4fd-821b202da95e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip install matplotlib numpy pillow opencv-python --no-index\n",
    "#!pip install scikit-learn tensorboard pytorch-lightning comet-ml --no-index\n",
    "#!pip install --upgrade --force-reinstall torch torchvision pandas --no-index"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37d1d663-01ec-448b-8233-c4b91c1806de",
   "metadata": {},
   "source": [
    "## Imports and Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6699ded-140e-4576-bd3b-618a9ab5a07a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "import cv2\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "from glob import glob\n",
    "from tqdm import tqdm, trange\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import pytorch_lightning as pl\n",
    "import sys\n",
    "\n",
    "from lit_model import lit_gazetrack_model\n",
    "\n",
    "from model import gazetrack_model\n",
    "from gazetrack_data import gazetrack_dataset\n",
    "\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from torchvision.transforms import Normalize, Resize, Compose, ToTensor, RandomCrop\n",
    "root = os.environ['SLURM_TMPDIR']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "29c411df-66cc-4fb0-a5c0-7e8f085f401b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "62fcf51a-680f-44f6-92b8-63edd1043640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/abhinav1/projects/def-skrishna/beluga_backup/gaze-track/Checkpoints/GoogleCheckpoint_MITSplit.ckpt\n",
      "/home/abhinav1/Users/01046/images/ 946\n"
     ]
    }
   ],
   "source": [
    "f = '/home/abhinav1/Users/01046/images/'#Path of the User containing images and meta\n",
    "weight_file = 'load_weights'\n",
    "print(weight_file)\n",
    "print(f, len(glob(f+'*.jpg')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7c7787a1-8846-4b7c-bfa0-67dd56068906",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gazetrack_model(\n",
       "  (eye_model): eye_model(\n",
       "    (model): Sequential(\n",
       "      (0): Conv2d(3, 32, kernel_size=(7, 7), stride=(2, 2))\n",
       "      (1): BatchNorm2d(32, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (3): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (4): Dropout(p=0.02, inplace=False)\n",
       "      (5): Conv2d(32, 64, kernel_size=(5, 5), stride=(2, 2))\n",
       "      (6): BatchNorm2d(64, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (7): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (8): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (9): Dropout(p=0.02, inplace=False)\n",
       "      (10): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1))\n",
       "      (11): BatchNorm2d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (12): LeakyReLU(negative_slope=0.01, inplace=True)\n",
       "      (13): AvgPool2d(kernel_size=2, stride=2, padding=0)\n",
       "      (14): Dropout(p=0.02, inplace=False)\n",
       "    )\n",
       "  )\n",
       "  (lmModel): landmark_model(\n",
       "    (model): Sequential(\n",
       "      (0): Linear(in_features=8, out_features=128, bias=True)\n",
       "      (1): BatchNorm1d(128, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (2): ReLU(inplace=True)\n",
       "      (3): Linear(in_features=128, out_features=16, bias=True)\n",
       "      (4): BatchNorm1d(16, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (5): ReLU(inplace=True)\n",
       "      (6): Linear(in_features=16, out_features=16, bias=True)\n",
       "      (7): BatchNorm1d(16, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "      (8): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (combined_model): Sequential(\n",
       "    (0): Linear(in_features=1040, out_features=8, bias=True)\n",
       "    (1): BatchNorm1d(8, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (2): Dropout(p=0.12, inplace=False)\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): Linear(in_features=8, out_features=4, bias=True)\n",
       "    (5): BatchNorm1d(4, eps=1e-05, momentum=0.9, affine=True, track_running_stats=True)\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Linear(in_features=4, out_features=2, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Loading the model\n",
    "model = gazetrack_model()\n",
    "if(torch.cuda.is_available()):\n",
    "    dev = torch.device('cuda:0')\n",
    "else:\n",
    "    dev = torch.device('cpu')\n",
    "    \n",
    "w = torch.load('/home/abhinav1/projects/def-skrishna/beluga_backup/gaze-track/Checkpoints/GoogleCheckpoint_MITSplit.ckpt',map_location=dev)['state_dict']\n",
    "model.load_state_dict(w)\n",
    "model.to(dev)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "35aad676-ed7f-4c98-939e-eadb0ea69d5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.utils.hooks.RemovableHandle at 0x2af87a2feb20>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Add hook to [5] for pre ReLU and [6] for after ReLU\n",
    "activation = {}\n",
    "def get_activation(name):\n",
    "    def hook(model, input, output):\n",
    "        activation[name] = output.detach()\n",
    "    return hook\n",
    "model.combined_model[6].register_forward_hook(get_activation('out'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "e64a019a-14c4-4f0a-ace7-b864838a032b",
   "metadata": {},
   "outputs": [],
   "source": [
    "data=[]\n",
    "for j in os.listdir('/home/abhinav1/Users_MIT/02078/images'):#Iterating by specifying the User ID\n",
    "\n",
    "    if(j.endswith('.jpg')):\n",
    "        last4 = j[0:-4]\n",
    "        current_images='/home/abhinav1/Users_MIT/02078/images'\n",
    "        cur_meta='/home/abhinav1/Users_MIT/02078/meta'\n",
    "        path=os.path.join(current_images,j)\n",
    "        image = Image.open(path)\n",
    "        w, h = image.size\n",
    "        last4=last4+'.json'\n",
    "        json_path=os.path.join(cur_meta,last4)\n",
    "        #\n",
    "        with open(json_path, 'r') as f:\n",
    "              meta = json.load(f)\n",
    "        screen_w, screen_h = meta['screen_w'], meta['screen_h']\n",
    "        lx, ly, lw, lh = meta['leye_x'], meta['leye_y'], meta['leye_w'], meta['leye_h']\n",
    "        rx, ry, rw, rh = meta['reye_x'], meta['reye_y'], meta['reye_w'], meta['reye_h']\n",
    "\n",
    "        kps = [meta['leye_x1']/w, meta['leye_y1']/h, meta['leye_x2']/w, meta['leye_y2']/h, \n",
    "               meta['reye_x1']/w, meta['reye_y1']/h, meta['reye_x2']/w, meta['reye_y2']/h]\n",
    "\n",
    "        l_eye = image.crop((max(0, lx), max(0, ly), max(0, lx+lw), max(0, ly+lh)))\n",
    "        r_eye = image.crop((max(0, rx), max(0, ry), max(0, rx+rw), max(0, ry+rh)))\n",
    "\n",
    "        l_eye = l_eye.transpose(Image.FLIP_LEFT_RIGHT)\n",
    "\n",
    "        kps = torch.tensor(kps).float()\n",
    "\n",
    "        out = torch.tensor([meta['dot_xcam'], meta['dot_y_cam']]).float()\n",
    "\n",
    "        list_transforms = [Resize((128,128)),\n",
    "                           ToTensor(),\n",
    "                           Normalize(mean=(0.3741, 0.4076, 0.5425), std=(0.02, 0.02, 0.02)),]\n",
    "\n",
    "        list_trfms = Compose(list_transforms)\n",
    "\n",
    "        l_eye=list_trfms(l_eye)\n",
    "        r_eye=list_trfms(r_eye)\n",
    "        l_eye = l_eye[None]\n",
    "        l_eye=l_eye.cuda()\n",
    "        r_eye = r_eye[None]\n",
    "        r_eye=r_eye.cuda()\n",
    "\n",
    "        kps.shape\n",
    "        kps=kps[None]\n",
    "        kps.shape\n",
    "        target=out\n",
    "        target=target.cuda()\n",
    "        target=target[None]\n",
    "        kps=kps.cuda()\n",
    "        with torch.no_grad():\n",
    "            #pred = model(l_eye, r_eye, kps)\n",
    "            pred_penultimate = list(model(l_eye, r_eye, kps).cpu().detach().numpy())\n",
    "        pred_penultimate = list(activation[\"out\"].detach().cpu().numpy())\n",
    "        #pred=pred.cpu().detach().numpy()\n",
    "        #pred=np.array(pred)\n",
    "        pred_penultimate=np.array(pred_penultimate)\n",
    "        target=target.cpu().detach().numpy()\n",
    "        data.append([j,[pred_penultimate],[target]])\n",
    "\n",
    "        #preds.extend(pred)\n",
    "\n",
    "\n",
    "        #totals.extend(target.cpu().detach().numpy())\n",
    "        #list_pred.append((j,pred))\n",
    "        #target_pred.append((j,target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a39fc33-5120-4b2b-bbef-427b1b8a1c50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['02078__00446.jpg',\n",
       " [array([[1.4355229 , 3.08893   , 1.4311185 , 0.92534053]], dtype=float32)],\n",
       " [array([[  1.6516391, -10.521687 ]], dtype=float32)]]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "683f8fa5-36fc-4cae-8e15-9bf99916c089",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame(data,columns=['Image_ID','Penultimate_Output','GT_Value'])\n",
    "path_csv = '/home/abhinav1/Users_MIT/CSVs_MIT_Dinesh/02078.csv'\n",
    "df.to_csv(path_csv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "44662cc1-a817-4ab7-9d16-b6f3078acbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(model(l_eye, r_eye, kps).cpu().detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "fbf86f7c-69a1-4590-973f-9b03aaddba7a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1.0931695, -11.140883 ], dtype=float32)]"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "259fccae-f28e-4c8d-a91a-02b95539c15e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = list(activation[\"out\"].detach().cpu().numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "aea73722-5dd4-46fd-b7c1-40db86291456",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([1.0004677, 2.8215249, 2.4949894, 1.2041379], dtype=float32)]"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "1fa76982-938b-46cf-beaf-3872c487d4f5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1.0640, -10.5840])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "19989e98-b386-4116-bcee-ab984fd1ad2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([  1.0931695, -11.140883 ], dtype=float32)]"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dba1e3fa-e7ed-4cbe-84f4-b831c475c615",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
